{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4365b000",
   "metadata": {},
   "source": [
    "# Week 6 Assignment: File Ingestion Pipeline with PySpark\n",
    "This notebook:\n",
    "1. Generates sample CSV files for three file types\n",
    "2. Ingests them with PySpark\n",
    "3. Extracts date metadata from filenames\n",
    "4. Writes out the data (simulate truncate-load)\n",
    "No manual uploads neededâ€”just run and download your outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb33091",
   "metadata": {},
   "source": [
    "## 1. Install and configure PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update -qq\n",
    "!apt-get install openjdk-8-jdk-headless -qq\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257fae8",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive and set up directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703cf987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Mount Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set data and output directories\n",
    "DATA_DIR = '/content/drive/MyDrive/Week6Data'\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/Week6Output'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Data directory:\", DATA_DIR)\n",
    "print(\"Output directory:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b30493",
   "metadata": {},
   "source": [
    "## 3. Generate sample CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample CUST_MSTR files for two dates\n",
    "dates = ['20191112', '20191113']\n",
    "for d in dates:\n",
    "    df = pd.DataFrame({\n",
    "        'CustID': [1, 2, 3],\n",
    "        'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Age': [30, 25, 35]\n",
    "    })\n",
    "    df.to_csv(f\"{DATA_DIR}/CUST_MSTR_{d}.csv\", index=False)\n",
    "\n",
    "# Generate sample master_child_export files for two dates\n",
    "for d in dates:\n",
    "    mdf = pd.DataFrame({\n",
    "        'ParentID': [1, 2, 3],\n",
    "        'ChildID': [10, 20, 30],\n",
    "        'Value': [100.0, 200.5, 300.75]\n",
    "    })\n",
    "    mdf.to_csv(f\"{DATA_DIR}/master_child_export-{d}.csv\", index=False)\n",
    "\n",
    "# Generate sample H_ECOM_ORDER file\n",
    "odf = pd.DataFrame({\n",
    "    'OrderID': [1001, 1002, 1003],\n",
    "    'CustomerID': [1, 2, 3],\n",
    "    'Amount': [250.5, 100.0, 450.75]\n",
    "})\n",
    "odf.to_csv(f\"{DATA_DIR}/H_ECOM_ORDER.csv\", index=False)\n",
    "\n",
    "print(\"Sample files created:\") \n",
    "print(os.listdir(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde98f5c",
   "metadata": {},
   "source": [
    "## 4. Start Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, to_date, input_file_name, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Week6_FileIngestion\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b76d83",
   "metadata": {},
   "source": [
    "## 5. Read CSV files by pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CUST_MSTR files\n",
    "cust_df = spark.read.csv(f\"{DATA_DIR}/CUST_MSTR_*.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Read master_child_export files\n",
    "mc_df = spark.read.csv(f\"{DATA_DIR}/master_child_export-*.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Read H_ECOM_ORDER file\n",
    "orders_df = spark.read.csv(f\"{DATA_DIR}/H_ECOM_ORDER.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"CUST_MSTR count:\", cust_df.count())\n",
    "print(\"master_child_export count:\", mc_df.count())\n",
    "print(\"H_ECOM_ORDER count:\", orders_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800ff5d",
   "metadata": {},
   "source": [
    "## 6. Extract date information from filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fdf02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date for CUST_MSTR\n",
    "cust_df = cust_df.withColumn(\"src_file\", input_file_name()) \\\n",
    "    .withColumn(\"FileDate\", to_date(regexp_extract(col(\"src_file\"), r\"CUST_MSTR_(\\d{8})\\\\.csv$\", 1), \"yyyyMMdd\"))\n",
    "\n",
    "# Extract date and date key for master_child_export\n",
    "mc_df = mc_df.withColumn(\"src_file\", input_file_name()) \\\n",
    "    .withColumn(\"DateKey\", regexp_extract(col(\"src_file\"), r\"master_child_export-(\\d{8})\\\\.csv$\", 1)) \\\n",
    "    .withColumn(\"FileDate\", to_date(col(\"DateKey\"), \"yyyyMMdd\"))\n",
    "\n",
    "# Orders - include filename column for traceability\n",
    "orders_df = orders_df.withColumn(\"src_file\", input_file_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0675b0",
   "metadata": {},
   "source": [
    "## 7. Show schemas and sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae3cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CUST_MSTR Schema & Sample:')\n",
    "cust_df.printSchema()\n",
    "cust_df.show(5)\n",
    "\n",
    "print('master_child_export Schema & Sample:')\n",
    "mc_df.printSchema()\n",
    "mc_df.show(5)\n",
    "\n",
    "print('H_ECOM_ORDER Schema & Sample:')\n",
    "orders_df.printSchema()\n",
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a730c11",
   "metadata": {},
   "source": [
    "## 8. Write output (truncate-load simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166641a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite mode simulates daily truncate-load\n",
    "cust_df.write.mode(\"overwrite\").parquet(f\"{OUTPUT_DIR}/CUST_MSTR\")\n",
    "mc_df.write.mode(\"overwrite\").parquet(f\"{OUTPUT_DIR}/master_child\")\n",
    "orders_df.write.mode(\"overwrite\").parquet(f\"{OUTPUT_DIR}/H_ECOM_Orders\")\n",
    "\n",
    "print(\"Data written to:\")\n",
    "print(f\"- {OUTPUT_DIR}/CUST_MSTR\")\n",
    "print(f\"- {OUTPUT_DIR}/master_child\")\n",
    "print(f\"- {OUTPUT_DIR}/H_ECOM_Orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ebd527",
   "metadata": {},
   "source": [
    "## 9. Next Steps: Automation\n",
    "- Convert this notebook to a Python script:\n",
    "  ```bash\n",
    "  jupyter nbconvert --to script week6_assignment_complete.ipynb\n",
    "  ```\n",
    "- Schedule via `cron` on your local machine or using GitHub Actions for daily runs.\n",
    "- When migrating to Azure Data Factory, map each code block to ADF activities."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
