# -*- coding: utf-8 -*-
"""week5_assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iyHZANVOyDrGgsjKCj7n2-PqFKyMWzrw
"""

# 1. Install Java & SQLite CLI
!apt-get update -qq
!apt-get install -qq openjdk-11-jdk-headless sqlite3

# 2. Download Spark and unpack
!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
!tar xf spark-3.4.1-bin-hadoop3.tgz

# 3. Download SQLite JDBC driver
!wget -q https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.40.0.0/sqlite-jdbc-3.40.0.0.jar

# 4. Install PySpark, Avro support, pandas, and watchdog
!pip install -q pyspark avro-python3 pandas watchdog

import pandas as pd
import sqlite3

# Define two small sample tables
customers = pd.DataFrame({
    "customer_id": [1, 2, 3],
    "first_name":  ["Alice", "Bob", "Charlie"],
    "last_name":   ["Anderson", "Brown", "Clark"],
    "country":     ["USA", "Canada", "UK"]
})

orders = pd.DataFrame({
    "order_id":    [101, 102, 103, 104],
    "customer_id": [1,   2,   3,   1],
    "order_date":  ["2025-06-01", "2025-06-03", "2025-06-07", "2025-06-09"],
    "total":       [250.0, 125.5, 320.75, 80.0]
})

# Write into SQLite file
conn = sqlite3.connect("/content/sample.db")
customers.to_sql("customers", conn, if_exists="replace", index=False)
orders.to_sql("orders", conn, if_exists="replace", index=False)
conn.close()

# Verify tables
print("Tables in /content/sample.db:")
!sqlite3 /content/sample.db ".tables"

import os
from pyspark.sql import SparkSession

# Point to Java & Spark
os.environ["JAVA_HOME"]  = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.1-bin-hadoop3"

# Build session with Avro support and SQLite driver on both driver & executor
spark = SparkSession.builder \
    .appName("Week5Assignment") \
    .config("spark.jars", "/content/sqlite-jdbc-3.40.0.0.jar") \
    .config("spark.driver.extraClassPath", "/content/sqlite-jdbc-3.40.0.0.jar") \
    .config("spark.jars.packages", "org.apache.spark:spark-avro_2.12:3.4.1") \
    .getOrCreate()

print("Spark version:", spark.version)

from pyspark.sql import DataFrame

def export_table_formats(df: DataFrame, table_name: str, out_base: str="/content/output"):
    """
    Writes df to CSV, Parquet, and Avro under:
      {out_base}/{table_name}/csv, /parquet, /avro
    """
    csv_path     = f"{out_base}/{table_name}/csv"
    parquet_path = f"{out_base}/{table_name}/parquet"
    avro_path    = f"{out_base}/{table_name}/avro"

    df.write.mode("overwrite").option("header","true").csv(csv_path)
    df.write.mode("overwrite").parquet(parquet_path)
    df.write.mode("overwrite").format("avro").save(avro_path)

    print(f"[{table_name}] exported to:\n  CSV ⮕ {csv_path}\n  Parquet ⮕ {parquet_path}\n  Avro ⮕ {avro_path}\n")

from pyspark.sql import Row

# create a tiny in‑memory DF
test_df = spark.createDataFrame([Row(a=1, b="x"), Row(a=2, b="y")])

# call the export helper
export_table_formats(test_df, "test_table")

# 1) Discover all tables via a subquery on sqlite_master
tables_df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:sqlite:/content/sample.db") \
    .option("dbtable", "(SELECT name AS table_name FROM sqlite_master WHERE type='table') AS tmp") \
    .option("driver", "org.sqlite.JDBC") \
    .load()

tables = [row.table_name for row in tables_df.collect()]
print("Found tables:", tables)

# 2) Loop through each table, replicate to dest.db, and export formats
dest_url = "jdbc:sqlite:/content/dest.db"
for tbl in tables:
    # Read source table
    df = spark.read \
        .format("jdbc") \
        .option("url", "jdbc:sqlite:/content/sample.db") \
        .option("dbtable", tbl) \
        .option("driver", "org.sqlite.JDBC") \
        .load()

    # Write back into dest.db (full replication)
    df.write \
      .format("jdbc") \
      .option("url", dest_url) \
      .option("dbtable", tbl) \
      .option("driver", "org.sqlite.JDBC") \
      .mode("overwrite") \
      .save()

    # Export CSV/Parquet/Avro of this table
    export_table_formats(df, tbl)

# Define the tables & columns you want to copy
whitelist = {
    "customers": ["customer_id", "first_name", "last_name", "country"],
    "orders":    ["order_id",    "customer_id",  "order_date", "total"]
}

for tbl, cols in whitelist.items():
    # Read only the specified columns from each table
    df_sel = spark.read \
        .format("jdbc") \
        .option("url", "jdbc:sqlite:/content/sample.db") \
        .option("dbtable", tbl) \
        .option("driver", "org.sqlite.JDBC") \
        .load() \
        .select(*cols)

    print(f"[Selective] {tbl} → columns {cols}")
    # Export them using your helper
    export_table_formats(df_sel, f"{tbl}_selective")

import os, time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

# 1) Prepare a watch folder
INPUT_DIR = "/content/input"
os.makedirs(INPUT_DIR, exist_ok=True)

# 2) Define the handler that reacts to new CSVs
class NewFileHandler(FileSystemEventHandler):
    def on_created(self, event):
        # Only act on new CSV files
        if not event.is_directory and event.src_path.endswith(".csv"):
            tbl = os.path.basename(event.src_path).replace(".csv", "")
            print(f"Detected new file: {event.src_path} → loading as '{tbl}'")
            df = spark.read.option("header", "true").csv(event.src_path)
            export_table_formats(df, f"evt_{tbl}")

# 3) Start the observer
observer = Observer()
observer.schedule(NewFileHandler(), INPUT_DIR, recursive=False)
observer.start()

print(f"Watching {INPUT_DIR} for new CSVs. (Upload one to trigger export.)")

# 4) Keep it alive long enough for testing (adjust sleep duration as needed)
try:
    time.sleep(300)   # 5 minutes
finally:
    observer.stop()
    observer.join()

